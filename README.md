# Conversational Insurance AI Platform With RAG
A FastAPI-based, chatbot-first insurance assistant that combines policy database grounding with FAQ retrieval for reliable responses.

## Problem Statement
Many insurance chatbots generate fluent but inaccurate answers, especially for policy-specific questions. In insurance support, hallucinated policy numbers, coverage limits, or status details are unacceptable.

This MVP addresses that issue by treating the policy database as the source of truth and using retrieval only for general insurance explanation.

## Key Differentiators
- Hybrid grounding: structured SQL policy data + FAQ retrieval in one response pipeline.
- Policy-first reliability: policy facts are fetched from database records, not generated by the model.
- Multi-policy handling: users with multiple active policies are asked to choose the target policy.
- Built-in insurance purchase flow: browse products and buy policies with optional add-ons.
- Chatbot-first UX: all major actions are available from the conversational interface.

## System Architecture
```text
[Browser UI: HTML/CSS/Vanilla JS]
              |
              v
        [FastAPI API Layer]
              |
              +--> Auth (/signup, /login) --> JWT
              |
              +--> Chat (/chat)
              |      |
              |      +--> Policy Service --> SQLite (users, policies, coverage, addons)
              |      |
              |      +--> RAG Engine --> LlamaIndex + FAISS (faq.csv + uploaded CSVs)
              |      |
              |      '--> LLM Client --> Groq (primary) / Ollama (fallback)
              |
              +--> Product Service (/products, /buy-policy)
              |
              '--> Data Ingestion (/upload-data)
```

## Tech Stack
| Layer | Technologies |
|---|---|
| Backend | FastAPI, Pydantic, Uvicorn |
| Authentication | JWT (`python-jose`), Passlib/Bcrypt |
| Database | SQLite, SQLAlchemy ORM |
| AI / Retrieval | LlamaIndex, FAISS, SentenceTransformers |
| LLM | Groq API (primary), Ollama (optional fallback) |
| Frontend | HTML, CSS, Vanilla JavaScript |
| Data | CSV seed datasets (`users`, `policies`, `coverage_details`, `faq`) |

## Folder Structure
```text
backend/
  main.py
  database.py
  models.py
  auth.py
  chat_engine.py
  policy_service.py
  product_service.py
  seed_data.py
  requirements.txt
  rag/
    rag_engine.py
    vector_store.py
    data_loader.py

frontend/
  index.html
  style.css
  script.js

data/
  users.csv
  policies.csv
  coverage_details.csv
  faq.csv
  vector_store/
    faiss.index
    docstore.json
    index_store.json
    default__vector_store.json
    image__vector_store.json
    graph_store.json
```

## Core Features
- Policy-aware conversational support.
- JWT-based signup/login authentication.
- SQL-backed policy and coverage lookup.
- RAG-powered FAQ explanation (top-k retrieval).
- Multi-policy disambiguation prompt for authenticated users.
- Insurance product catalog and policy purchase APIs.
- Add-on packs per insurance type and policy.
- CSV upload endpoint for additional knowledge ingestion.
- Auto database bootstrap and CSV seed loading on startup.

## Chat Engine Logic
1. Receive user message, optional policy number, and optional JWT identity.
2. Detect intent (policy query, plan discovery, purchase request, add-on query).
3. Resolve policy target:
   - Explicit policy number from request/message, or
   - Auto-select if user has exactly one active policy, or
   - Ask user to choose if multiple active policies exist.
4. Validate policy existence and ownership (when authenticated).
5. Fetch structured policy data from database (policy, coverage, add-ons).
6. Retrieve top FAQ chunks from vector store for explanation support.
7. Build grounded prompt and call Groq (or Ollama fallback).
8. Append expiry guidance when policy is expired.
9. Return response plus policy metadata (`policy_number`, `requires_policy`, `booking_intent`).

## RAG Strategy
- `faq.csv` and uploaded CSVs are embedded and indexed in FAISS via LlamaIndex.
- Retrieval is used for **general insurance knowledge and explanation**.
- Policy-specific facts (status, premium, dates, coverage limits, exclusions) come from SQL records.
- If FAISS retrieval is unavailable, lexical fallback keeps the assistant functional.

## Setup Instructions
### 1) Create and activate virtual environment
```bash
python -m venv .venv
```

Windows:
```bash
.venv\Scripts\activate
```

macOS/Linux:
```bash
source .venv/bin/activate
```

### 2) Install dependencies
```bash
pip install -r backend/requirements.txt
```

### 3) Run the app
```bash
uvicorn backend.main:app --reload
```

If your local runner resolves the default app from module path, this shorthand can also be used:
```bash
uvicorn backend.main --reload
```

Open:
- App UI: `http://127.0.0.1:8000/`
- Swagger docs: `http://127.0.0.1:8000/docs`

## Environment Variables (`.env`)
```env
# JWT
JWT_SECRET_KEY=change-this-secret-before-production
JWT_ALGORITHM=HS256
JWT_EXPIRE_MINUTES=1440

# Groq (primary LLM)
GROQ_API_KEY=your_groq_api_key
GROQ_MODEL=llama-3.3-70b-versatile

# Ollama (optional fallback)
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1

# Database (optional override)
# DATABASE_URL=sqlite:///./insurance.db

# Embedding model (optional override)
# EMBED_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
```

## API Endpoints
| Method | Route | Auth | Purpose |
|---|---|---|---|
| GET | `/health` | No | Service health check |
| POST | `/signup` | No | Create user and return JWT |
| POST | `/login` | No | Authenticate user and return JWT |
| POST | `/chat` | Optional | Policy-aware chatbot response |
| GET | `/policy/{policy_number}` | Optional | Fetch policy detail (owner check if authenticated) |
| GET | `/products` | No | List insurance products and add-ons |
| POST | `/buy-policy` | Yes | Purchase policy for authenticated user |
| POST | `/upload-data` | Yes | Upload CSV and ingest into retrieval index |

## Demo Flow 
1. Start the application and open the web UI.
2. Login with seeded account (example: `alice@example.com / alice123`).
3. Ask: `I need policy details`.
4. Observe multi-policy prompt if the user has multiple active policies.
5. Provide a policy number and ask a coverage question.
6. Ask: `show available plans`.
7. Buy a plan from UI or chat and verify newly issued policy number.
8. Ask add-on related questions for the selected policy.
9. (Optional) Upload additional FAQ CSV via `/upload-data` and ask relevant questions.

## Anti-Hallucination Design
- **Policy database is the source of truth** for policy-level answers.
- Policy numbers are validated against database records before response generation.
- Coverage and policy metadata are injected into the LLM prompt from SQL output.
- RAG is used only for explanatory insurance knowledge, not authoritative policy facts.
- Ownership checks prevent authenticated users from reading other users' policies.

## Scalability Notes
- Database migration path: SQLite -> PostgreSQL by changing `DATABASE_URL`.
- Retrieval index can be externalized to managed vector infrastructure if required.
- Background jobs can be added for asynchronous ingestion and re-indexing.
- Optional Redis can be introduced for rate limiting, caching, and session support.
- API can be containerized and deployed behind a reverse proxy/load balancer.

## Operational Notes
- Database and seed data are initialized on startup.
- Product catalog and add-on defaults are seeded if missing.
- If Groq is unavailable, Ollama fallback is attempted when configured.
- If FAISS initialization fails, lexical retrieval fallback keeps chat usable.

